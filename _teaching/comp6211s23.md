---
permalink: /teaching/comp6211s23
title: ""
collection: teaching
# excerpt: "This is a page not in th emain menu"
author_profile: false
layout: teaching
---

# COMP 6211I： Trustworthy Machine Learning [Spring 2023]

Monday,  13:30-14:50 @ Room 6591

Friday, 9:00-10:20 @ Room 6591

## Overview
- Instructor: [Minhao Cheng](https://cse.hkust.edu.hk/~minhaocheng) (minhaocheng@cse.ust.hk)
    - Office hours:  @ Room 2542

- Teaching assistants: 
    - TBA
- Canvas: [COMP6211I](https://canvas.ust.hk/courses/47885)

## Announcements

## Description
This is an intensive graduate seminar on Trustworthy machine learning. The course covers different topics in emerging research areas related to the broader study of security and privacy in machine learning. Students will learn about attacks against computer systems leveraging machine learning, as well as defense techniques to mitigate such attacks.
## Prerequisites
The course assumes students already have a basic understanding of machine learning. Students will familiarize themselves with the emerging body of literature from different research communities investigating these questions. The class is designed to help students explore new research directions and applications. Most of the course readings will come from both seminal and recent papers in the field. 
## Grading Policy
- Paper presentation (25%)
- Paper summaries (10%)
- Class notes & participation (15%)
- Exam (15%)
- Research project (35%)

## Assignments
- **Reading summary:**
A 1 page summary of reading assigned is due each class (starting from week 2 and onwards). A physical copy should be turned in before the beginning of class. The summary should cover the following: (a) what did the papers do well?, (b) where did the papers fall short?, (c) what did you learn from these papers?, and (d) what questions do you have about the papers?
- **Paper presentation:**
starting from week 2, a team of students will present the papers assigned for reading each week. The team may choose an appropriate format (e.g., slides, interactive demos or code tutorials, ...) for this presentation with the only requirements being that the presentation should (a) involve the class in active discussions, (b) cover all papers assigned for reading, and (c) last no more than 1h30mn including discussions.
- **Class notes:**
Another team of students will be charged with writing notes synthesizing the content of the presentation and class discussion.

## Research Projects
Students are required to do a project in this class. The goal of the course project is to provide the students an opportunity to explore research directions in trustworthy machine learning. The project should be related to the course content. An expected project consists of

- A novel and sound solution to an interesting problem
- Comprehensive literature review and discussion
- Thorough theoretical/experimental evaluation and comparisons with existing approaches
## Tentative Schedule and Material


|  Date   | Topic                        | Slides | Readings&links | Assignments |
|  ----   | ----                         | ----   | ----           | ----        |
| Fri 3/2 | Overview of Trustworthy Machine Learning |        |                |             |
| Mon 6/2 | Machine learning basics      |        |    |             |
|         | Test-time intergrity (attack)      |        | White-box attack: <br> • [Goodfellow et al., Explaining and Harnessing Adversarial Examples](https://arxiv.org/pdf/1412.6572) <br> • [Carlinin and Wagner, Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/abs/1608.04644) <br> • [Moosavi-dezfooli et al., Universal adversarial perturbations](https://arxiv.org/abs/1610.08401) <br> Hard-label black-box attack: <br> • [Brendel et al., Decision-based adversarial attacks: reliable attacks against black-box machine learning models](https://openreview.net/pdf?id=SyZI0GWCZ) <br> • [Cheng et al., Query-efficient hard-label black-box attack: an optimization-based approach](https://openreview.net/pdf?id=rJlk6iRqKX) <br> • [Chen et al., HopSkipJumpAttack: A Query-Efficient Decision-Based Attack](https://arxiv.org/pdf/1904.02144.pdf) |             |
|         | Test-time intergrity (defense)      |  | • [Madry et al., Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/pdf/1706.06083.pdf) <br> • [Wong et al., Fast is better than Free: Revisiting Adversarial Training](https://arxiv.org/pdf/2001.03994.pdf) <br> • [Zhang et al., Theoretically Principled Trade-off between Robustness and Accuracy](https://arxiv.org/pdf/1901.08573.pdf)|                | 
|         | Test-time intergrity (verification)      |        | • [Eric and Kolter, Provable defenses against adversarial examples via the convex outer adversarial polytope](https://arxiv.org/pdf/1711.00851.pdf) <br> • [Zhang et al., Efficient Neural Network Robustness Certification with General Activation Functions](https://arxiv.org/pdf/1811.00866.pdf) <br> • [Zhang et al., General Cutting Planes for Bound-Propagation-Based Neural Network Verification](https://arxiv.org/pdf/2208.05740.pdf) <br> • [Cohen et al., Certified Adversarial Robustness via Randomized Smoot](https://arxiv.org/pdf/1902.02918.pdf)          |             |
|         | Training-time intergrity      |           |Attack: <br> • [Liu et al., Trojaning Attack on Neural Networks](https://people.cs.rutgers.edu/~sm2283/papers/NDSS18.TNN.pdf) <br> • [Koh and Liang, Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730.pdf) <br> • [Shafahi et al., Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks](https://arxiv.org/abs/1804.00792.pdf)<br> • [Carlini and Terzis, Poisoning and Backdooring Contrastive Learning](https://arxiv.org/abs/2106.09667.pdf) <br> • [Carlini, Poisoning the Unlabeled Dataset of Semi-Supervised Learning](https://nicholas.carlini.com/papers/2021_usenix_poisoningsemisupervised.pdf)<br> • [Gu et al., BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain](https://arxiv.org/abs/1708.06733.pdf)<br> • [Liu et al., Trojaning Attack on Neural Networks](https://people.cs.rutgers.edu/~sm2283/papers/NDSS18.TNN.pdf)<br> Detection&Defense: <br> • [Wang et al., Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks](https://people.cs.uchicago.edu/~ravenben/publications/pdf/backdoor-sp19.pdf) <br> • [Wang et al., Practical Detection of Trojan Neural Networks: Data-Limited and Data-Free Cases](https://arxiv.org/abs/2007.15802.pdf)|           |
|         | Safety | |• [Athalye et al., Synthesizing Robust Adversarial Examples](https://arxiv.org/pdf/1707.07397.pdf) <br> •  [Xu et al., Adversarial T-shirt! Evading Person Detectors in A Physical World](https://arxiv.org/abs/1910.11099.pdf)| |
|         | Confidentiality (model)     | | •  [Jagielski et al., High Accuracy and High Fidelity Extraction of Neural Networks](https://www.usenix.org/system/files/sec20-jagielski.pdf) <br> • [Tramer et al., Stealing Machine Learning Models via Prediction APIs](https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_tramer.pdf)      |                |        
|         | Confidentiality (data) |    | Attack: <br> • [Carlini et al., Extracting Training Data from Large Language Models](https://arxiv.org/abs/2012.07805.pdf)  <br> •  [Kahla et al., Label-Only Model Inversion Attacks via Boundary Repulsion](https://arxiv.org/abs/2203.01925.pdf)<br> Defense: <br> •  [Huang et al., Unlearnable Examples: Making Personal Data Unexploitable](https://arxiv.org/abs/2101.04898)  <br> • [Maini, Dataset Inference: Ownership Resolution in Machine Learning](https://arxiv.org/abs/2104.10706.pdf) |                |           
|         | Privacy attacks |  |• [Shokri et al., Membership Inference Attacks against Machine Learning Models](https://arxiv.org/abs/1610.05820.pdf)   <br> •  [Fredrikson et al., Model inversion attacks that exploit confidence information and basic countermeasures](https://rist.tech.cornell.edu/papers/mi-ccs.pdf) <br> •  [Choquette-Choo et al., Label-Only Membership Inference Attacks](http://proceedings.mlr.press/v139/choquette-choo21a/choquette-choo21a.pdf) |            |
|         | Differential privacy     |        | • [Dwork et al., Calibrating Noise to Sensitivity in Private Dat Analysis](https://people.csail.mit.edu/asmith/PS/sensitivity-tcc-final.pdf) • [Abadi et al., Deep Learning with Differential Privacy](https://arxiv.org/abs/1607.00133) <br> •  [Papernot et al., Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data](https://arxiv.org/abs/1610.05755.pdf)  <br> • [Mironov, Renyi Differential Privacy](https://arxiv.org/abs/1702.07476.pdf)           |             |
|         | Fairness    |        |• [Zhao et al., Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints](https://arxiv.org/abs/1707.09457.pdf)               |             |
|         | Interpretability (XAI) |   |• [Simonyan et al., Deep inside convolutional networks: Visualising image classication models and saliency maps](https://arxiv.org/abs/1312.6034.pdf)<br>• [Selvaraju et al., Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391.pdf)<br> • [Ribeiro et al., "Why Should I Trust You?": Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938.pdf) <br> • [Lundberg and Lee, A unified approach to interpreting model predictions](https://arxiv.org/abs/1705.07874.pdf)     |                |  


